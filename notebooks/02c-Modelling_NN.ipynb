{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-allen",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "grave-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from common import preprocess_text, eval_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required otherwise encounter CancelledError: [_Derived_]RecvAsync is cancelled\n",
    "# Reference: https://github.com/tensorflow/tensorflow/issues/33721\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-powder",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "permanent-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (153164, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "test_y = pd.read_csv('../data/test_labels.csv')\n",
    "test_df = pd.concat([test_df, test_y.iloc[:,1:]], axis=1, sort=False)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-harvey",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-trouble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_tracker = {}\n",
    "non_toxic_label = 'non_toxic'\n",
    "comment_col = 'comment_text'\n",
    "\n",
    "class_labels = train_df.columns.tolist()[2:]\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "middle-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-toxic class and fillna\n",
    "train_df[non_toxic_label] = 1 - train_df[class_labels].max(axis=1)\n",
    "train_df[comment_col] = train_df[comment_col].fillna('unknown')\n",
    "test_df[comment_col] = test_df[comment_col].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-award",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-facial",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-scene",
   "metadata": {},
   "source": [
    "#### 2. Neural Network Model\n",
    "- tensorflow keras is run with GPU here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-incidence",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protected-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_nn(train_df, test_df,\n",
    "                  comment_col=comment_col,\n",
    "                  class_labels=class_labels,\n",
    "                  max_features=20000,\n",
    "                  maxlen=100):\n",
    "    '''Tokenize and pad data for NN training'''\n",
    "    \n",
    "    train_df = train_df.sample(frac=1, random_state=123).copy()\n",
    "    train_x = train_df[comment_col].values\n",
    "    train_y = train_df[class_labels].values\n",
    "    test_x = test_df[comment_col].values\n",
    "    \n",
    "    # Tokenize data\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_x))\n",
    "    train_x = tokenizer.texts_to_sequences(train_x)\n",
    "    test_x = tokenizer.texts_to_sequences(test_x)\n",
    "    \n",
    "    train_seq = sequence.pad_sequences(train_x, maxlen=maxlen)\n",
    "    test_seq = sequence.pad_sequences(test_x, maxlen=maxlen)\n",
    "    \n",
    "    return tokenizer, train_y, train_seq, test_seq\n",
    "\n",
    "def get_embedding(emb_fp, tokenizer,\n",
    "                  max_features=20000,\n",
    "                  embed_size=128):\n",
    "    '''Get embedding matrix of training data\n",
    "    from embedding in emb_fp'''\n",
    "    \n",
    "    assert emb_fp is not None, 'emb_fp is not specified'\n",
    "    \n",
    "    def _get_coefs(word, *arr):\n",
    "        '''Generate dict of word-vector k-v pair'''\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    with open(emb_fp, encoding='utf-8') as file:\n",
    "        emb_index = dict(_get_coefs(*l.strip().split()) for l in file)\n",
    "    # Discard words with vector size not equals embed_size\n",
    "    # Observe this error when using Glove twitter embedding\n",
    "    discard_keys = [k for k, v in emb_index.items() if len(v)!=embed_size]\n",
    "    for k in discard_keys:\n",
    "        del emb_index[k]\n",
    "        \n",
    "    all_embs = np.stack(emb_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    \n",
    "    # Initialize emb matrix with normal distribution\n",
    "    # from mean and std of passed embbeding\n",
    "    np.random.seed(123)\n",
    "    emb_matrix = np.random.normal(emb_mean, emb_std,\n",
    "                                  (nb_words, embed_size))\n",
    "    \n",
    "    # Replace random init value with that from passed embedding\n",
    "    # if word is found\n",
    "    for word, idx in word_index.items():\n",
    "        if idx>=max_features:\n",
    "            continue\n",
    "        emb_vector = emb_index.get(word)\n",
    "        if emb_vector is not None:\n",
    "            emb_matrix[idx] = emb_vector\n",
    "    \n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seasonal-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(struc,\n",
    "                 embedding,\n",
    "                 max_features=20000,\n",
    "                 maxlen=100,\n",
    "                 embed_size=128,\n",
    "                 dropout=0.1):\n",
    "    '''Bidirectional GRU/LSTM with 2 fully connected layers\n",
    "    initialized with a embedding matrix (if specified)\n",
    "    with dropout'''\n",
    "    \n",
    "    assert struc in ['GRU','LSTM'], 'struc type not supported'\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    if embedding is not None:\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding])(inp)\n",
    "    else:\n",
    "        x = Embedding(max_features, embed_size)(inp)\n",
    "        \n",
    "    if struc=='LSTM':\n",
    "        x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    else:\n",
    "        x = Bidirectional(GRU(50, return_sequences=True))(x)\n",
    "        \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quantitative-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_nn(model, model_fp,\n",
    "                train_y, train_seq, test_seq,\n",
    "                batch_size=32, epochs=5):\n",
    "    '''Get predictions of NN models for each label at a time'''\n",
    "    \n",
    "    model_full_fp = os.path.abspath(model_fp)\n",
    "    model_dir = os.path.dirname(model_full_fp)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Define callbacks\n",
    "    checkpoint = ModelCheckpoint(model_fp, monitor='val_loss',\n",
    "                                 verbose=1, save_best_only=True,\n",
    "                                 mode='min')\n",
    "    early = EarlyStopping(monitor='val_loss',\n",
    "                          mode='min',\n",
    "                          patience=20)\n",
    "    callbacks_list = [checkpoint, early]\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(train_seq, train_y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.1,\n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    # Load best weights and make pred\n",
    "    model.load_weights(model_fp)\n",
    "    test_y = model.predict(test_seq)\n",
    "    \n",
    "    return test_y\n",
    "\n",
    "def run_nn(struc, model_fp,\n",
    "           train_df, test_df,\n",
    "           embed_fp=None,\n",
    "           embed_size=128,\n",
    "           max_features=20000,\n",
    "           maxlen=100,\n",
    "           dropout=0.1,\n",
    "           batch_size=32,\n",
    "           epochs=2,\n",
    "           class_labels=class_labels):\n",
    "    '''Run 1 NN prediction cycle'''\n",
    "    \n",
    "    # Preprocess\n",
    "    tokenizer, train_y, train_seq, test_seq = \\\n",
    "        preprocess_nn(\n",
    "            train_df=train_df,\n",
    "            test_df=test_df,\n",
    "            class_labels=class_labels,\n",
    "            max_features=max_features,\n",
    "            maxlen=maxlen)\n",
    "    print('1. Preprocessed completed\\n')\n",
    "    \n",
    "    # Get embeddings if specified\n",
    "    if embed_fp is not None:\n",
    "        embedding = get_embedding(\n",
    "            embed_fp,\n",
    "            tokenizer,\n",
    "            max_features=max_features,\n",
    "            embed_size=embed_size)\n",
    "        print('2. Embeddings generated\\n')\n",
    "    else:\n",
    "        embedding=None\n",
    "        print('2. No Embeddings\\n')\n",
    "    \n",
    "    # Generate model\n",
    "    model = get_nn_model(\n",
    "        struc=struc,\n",
    "        embedding=embedding,\n",
    "        max_features=max_features,\n",
    "        maxlen=maxlen,\n",
    "        embed_size=embed_size,\n",
    "        dropout=dropout)\n",
    "    print(f'3. Model generated ({struc})\\n')\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = get_pred_nn(\n",
    "        model, model_fp,\n",
    "        train_y, train_seq, test_seq,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    print('4. Predictions completed\\n')\n",
    "    \n",
    "    # Eval predictions\n",
    "    score = eval_pred(test_df, preds, class_labels)\n",
    "    print('5. Evaluation completed\\n')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-fiber",
   "metadata": {},
   "source": [
    "##### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "international-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embed_size = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Embedding filepaths\n",
    "glove_wiki_fp = '../glove/glove.6B/glove.6B.100d.txt'\n",
    "glove_twitter_fp = '../glove/glove.twitter.27B/glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-modern",
   "metadata": {},
   "source": [
    "##### 2.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed completed\n",
      "\n",
      "2. No Embeddings\n",
      "\n",
      "3. Model generated (LSTM)\n",
      "\n",
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 139s 30ms/step - loss: 0.1032 - accuracy: 0.8280 - val_loss: 0.0507 - val_accuracy: 0.9937\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05069, saving model to models\\weights_lstm.best.hdf5\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 137s 31ms/step - loss: 0.0451 - accuracy: 0.9852 - val_loss: 0.0479 - val_accuracy: 0.9938.0451 - accurac - ETA: 2 - ETA: 25s - loss: 0.0451 - accuracy: 0. - ETA: 25s - loss: 0.0451 - accuracy: 0.985 - ETA: 25s - loss: 0.0451 - accuracy: 0.985 - ETA: 25s - loss: 0.0451 - ETA: 24s - loss: 0.0451 - accuracy: 0. - ETA: 23s - loss: 0.0451 - accuracy: - ETA: 23s - loss: 0.0451 - accuracy: 0.9 - ETA: 23s - loss: 0.0451 - accuracy: 0.98 - ETA: 23s - loss: 0.0451 - accuracy:  - ETA: 22s - loss: 0.0451 - accuracy: 0 - ETA: 22s - loss: 0.0451 - acc - ETA: 21s - loss: 0.0451 - accuracy: 0.98 - ETA: 21s - loss: 0.0451 - accuracy:  - ETA: 21s - loss: 0.0451 - accura - ETA: 20s - loss: 0.0451 - a - ETA: 19s - loss: 0.0451 - accuracy: 0.9 - ETA: 19s - loss: 0.0451 - accura - ETA: 18s - loss: 0.0451 - accurac - ETA: 18s - loss: 0.0451 - accuracy: 0 - ETA: 18s - loss: 0.0451 -  - ETA: 17s - loss: 0.0451 - accuracy: 0. - ETA: 16s - loss:  - ETA: 15s - loss: 0.0451 - accuracy: 0. - ETA: 15s - loss: 0.0451 - accuracy:  - ETA: 14s - loss: 0.0451 - accuracy: 0.9 - ETA: 14s - loss: 0.0451 - accuracy:  - ETA: 14s - loss: 0.0451 - accuracy: 0.98 - ETA: 14s - loss: 0.0451 - acc - ETA: 13s - loss: 0.0451 - accuracy: 0.985 - ETA: 13s - loss: 0.0451 - accuracy: 0.985 - ETA:  - ETA: 10s - loss: 0.0451 - accuracy: 0.9 - ETA: 10s - loss: 0. - ETA: 9s - loss: 0.0451 - accuracy - ETA: 9s - loss: 0.0451 - accuracy: 0. - ETA: 9s - loss: 0.045 - ETA: 8s - loss: 0.0451 - accura - ETA: 8s - loss: 0.0451 - accuracy:  - ETA: 8s - loss: 0.0451 - accuracy: 0. - ETA: 8s - loss: 0.0451 - accuracy - ETA: 7s - loss: 0.0451 -  - ETA: 7s - loss: 0.0451 - accu - ETA: 6s - loss: 0.0451 - accuracy: 0.98 - ETA: 6s - loss: 0.0451  - ETA: 6s - loss: 0.0451 - ac - ETA: 5s - loss: 0.045\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05069 to 0.04786, saving model to models\\weights_lstm.best.hdf5\n",
      "4. Predictions completed\n",
      "\n",
      "Mean ROC-AUC: 0.9741297926161975\n",
      "5. Evaluation completed\n",
      "\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_lstm = run_nn(\n",
    "    struc='LSTM',\n",
    "    model_fp=r'models/weights_lstm.best.hdf5',\n",
    "    train_df=train_df, test_df=test_df,\n",
    "    embed_fp=None,\n",
    "    embed_size=embed_size,\n",
    "    max_features=max_features,\n",
    "    maxlen=maxlen,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_labels=class_labels)\n",
    "scores_tracker['nn_lstm'] = score_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-burden",
   "metadata": {},
   "source": [
    "##### 2.2 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "characteristic-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed completed\n",
      "\n",
      "2. No Embeddings\n",
      "\n",
      "3. Model generated (GRU)\n",
      "\n",
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 142s 31ms/step - loss: 0.0929 - accuracy: 0.8949 - val_loss: 0.0494 - val_accuracy: 0.9935ccuracy: 0.80 - ETA: 1:28 - loss: 0.1 - ETA: 1:27 - loss: 0.1368 - accuracy: 0.81 - ETA: 1:27 - - ETA: 1:24 - loss: 0.1325 - accuracy - ETA: 1:23 - loss: 0.1323 - accuracy - ETA: 1:23 - loss: 0.1320 - accuracy - ETA: 1:23 - loss: 0.1318 - accuracy - - ETA: 1:21 - loss: 0.1302 - accuracy: 0.82 - ETA: 1:21 - loss: 0.1301 - accuracy:  - ETA: 1:21 - loss: 0.1300 - accuracy - ETA: 1:21 - loss: 0.1297 -  - ETA: 1:20 - ETA: 1:19 - loss: 0.1283 - accura - ETA: 1:19 - los - ETA: 1:18 - loss: 0.1272 - accura - ETA: 1:18 - loss: 0.1270 - accuracy: 0. - ETA - ETA: 1:17 - loss: 0.1258 - accu - ETA: 1:16 - loss: 0.1255 - accura - ETA: 1:16 - loss: 0.1252 - accuracy: 0.83 - ETA: 1:16 - - ETA: 1:15 - loss: 0.1244 - accuracy: 0. - ETA - ETA: 0s - loss: 0.0930 - ac - ETA: 0s - loss: 0.0929 - accuracy\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04936, saving model to models\\weights_gru.best.hdf5\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 141s 32ms/step - loss: 0.0440 - accuracy: 0.9642 - val_loss: 0.0465 - val_accuracy: 0.9928accu - ETA: 17s - loss: 0.0440 - accuracy: - ETA: 17s - loss: 0.0440 - accuracy: 0.963 - ETA: 17s - loss: 0.0440 - accuracy - ETA: 16s - loss: 0.044 - ETA: 15s - loss: 0.0440 - accuracy: 0.9 - ETA: 15s - loss: 0.0440 - accuracy: 0. - ETA: 15s - l - ETA: 13s - loss: 0.0440 - accuracy: 0.9 - ETA: 13s - loss: 0.0440 - accuracy: 0.96 - ETA: 13s - loss: 0.0440 - accuracy: - ETA: 12s - loss: 0.0440 - accuracy: 0.96 - ETA: 12s - loss: 0.0440 - accuracy: 0.9 - ETA: 12s - loss: 0.0440 -  - ETA: 11s - loss: 0.0440 - accuracy: - ETA: 11s - loss: 0.0440 - accuracy:  - ETA: 10s - loss: 0.0440 - accuracy: 0. - ETA: 10s - loss: 0.0440 - accuracy: 0. - ETA: 10s - loss: 0.0440 - accuracy: - ETA: 8s - loss: 0.0440 - accuracy: 0.96 - ETA: 8s - loss: 0 - ETA: 7s - loss: 0.0440 - accuracy - ETA: 7s - loss: 0.0440 - accuracy - ETA: 7s - loss: 0.0440 - accuracy: 0. - ETA: 7s - loss: 0 - ETA: 6s - l - ETA: 5s - loss: 0.0440 - accuracy: 0.96 - ETA: 5s - loss: 0.0440 - accuracy:  - ETA: 5s - loss: 0.0440 -  - ETA: 4s - loss: 0.0440 - accuracy:  - ETA: 4s - loss: 0.0440 - accura - ETA: 4s - ETA: 3s - loss: 0.0440 - accuracy: 0.96 - ETA: 3s - loss: 0.0440 - accuracy - ETA: 2s - loss: 0 - ETA: 2s - loss: 0.0440 - accuracy: 0.96 - ETA: 2s - loss: 0.0440 - accuracy: 0.96 - ETA: 1s - loss: 0.0440 - accuracy: 0.96 - ETA: 1s - loss: 0.0440 - accu - ETA: 1s - loss: 0.0440 - accu - ETA: 1s - loss: 0.0440 - accuracy: 0.96 - ETA: 1s - loss: 0.0440 - accuracy: 0.96 - ETA: 1s - loss: 0.0440 - accu - ETA: 0s - loss: 0.0440 - ac - ETA: 0s - loss: 0.0440 - accura\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04936 to 0.04654, saving model to models\\weights_gru.best.hdf5\n",
      "4. Predictions completed\n",
      "\n",
      "Mean ROC-AUC: 0.9745754951558911\n",
      "5. Evaluation completed\n",
      "\n",
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_gru = run_nn(\n",
    "    struc='GRU',\n",
    "    model_fp=r'models/weights_gru.best.hdf5',\n",
    "    train_df=train_df, test_df=test_df,\n",
    "    embed_fp=None,\n",
    "    embed_size=embed_size,\n",
    "    max_features=max_features,\n",
    "    maxlen=maxlen,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_labels=class_labels)\n",
    "scores_tracker['nn_gru'] = score_gru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-modeling",
   "metadata": {},
   "source": [
    "##### 2.3 GRU with Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleasant-bailey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxh88\\AppData\\Local\\Continuum\\anaconda3\\envs\\toxicc-env\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Embeddings generated\n",
      "\n",
      "3. Model generated (GRU)\n",
      "\n",
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 141s 31ms/step - loss: 0.0776 - accuracy: 0.8474 - val_loss: 0.0466 - val_accuracy: 0.9937:  - ETA: 52 - ETA: 47s - loss: 0.0882 - accuracy: 0 - ETA: 47s - loss: 0.0881 - accura - ETA: 47s - loss: 0.0879 - accuracy: 0.815 - ETA: 47s - loss: 0 - ETA: 45s - loss: 0.0875 - accur - ETA: 45s - loss: 0.0873 - a - ETA: 44s - loss: 0.0870 - accuracy: 0.817 - ETA: 44s - loss: 0.0870 - ETA: 43s - loss - ETA: 41s - loss: 0.0862 - accuracy: 0 - ETA: 41s - loss: 0.0861 - accu - ETA: 40s - loss: 0.0859 - ac - ETA: 39s - loss: 0.0857 - - ETA: 38s - loss: 0.0854 - accuracy: 0.822 - ETA: 38s - loss: 0.0854 - - ETA: 37s - loss: 0.0851 - accura - ETA: 36 - ET - ETA: 32s - loss: 0.0838 - a - ETA: 31 - ETA: 29s - loss: 0.0832 -  - ETA: 28s - loss: 0.0829 - accuracy:  - ETA: 28s - loss - ETA: 26s - loss: 0.0825 - acc - ETA: 25s - loss: 0.0823 - accuracy: 0. - ETA: 25s - loss: 0.0823 - accuracy: 0.8 - ETA: 25s - loss: 0.0822 - accuracy:  - ETA: 25s - loss: 0.0822 - accuracy: 0.832 - ETA: 24s - loss: 0.0821 - accuracy: 0.83 - ETA: 24s - loss: 0.0821 - accuracy: 0.832 - ETA: 24s - loss: 0.0821 - accuracy: 0.8 - ETA: 24s - loss: 0.0821 - \n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04664, saving model to models\\weights_gru_glove_wiki.best.hdf5\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 139s 31ms/step - loss: 0.0423 - accuracy: 0.9544 - val_loss: 0.0450 - val_accuracy: 0.9929- ETA - ETA: 1:28 - l - ETA: 1:27 - l - ETA: 1:26 - loss: - ETA: 1:24 - loss: 0.0419 - accuracy: 0. - ETA: 1:24 - loss: 0.0419 - accuracy: 0.96 - ETA: 1:24 - loss: 0 - ETA: 1:23 - loss: 0.0419 - accuracy:  - ETA: 1:23 - loss: 0.0419 -  - ETA: 1:21 - los - ETA: 1:21 - loss: 0.0419 - accuracy:  - ETA: 1:20 - loss: 0.0419  - ETA: 1:20 - loss: 0.0419 -  - ETA: 1:19 - loss: 0.0420 - accuracy:  - ETA: 1:19 - loss: 0.0420 - accuracy: 0.96 - ETA: 1:19 - los - ETA: 1:17 - loss: 0.0420 - accuracy: 0.96 - ETA: 1:17 - loss: 0.0420 - accuracy:  - ETA: 1:17 - loss: 0.0420 - ac - ETA: 1:16 - loss: 0.0420 - ac - ETA: 1:16 - loss: 0.0420 - accuracy: 0.96 - ETA: 1:16 - loss: 0.0420 - ac - ETA: 1:16 - loss: 0.0420 - accuracy: 0.96 - ETA: 1:16 - loss: 0.0420 - accura - ETA: 1:15 - l - ETA: 1:14 - loss: 0.0420 - accuracy: 0.95 - ETA: 1:14 - loss: 0.0420 - accuracy: 0.95 - ETA: 1:14 - loss: 0.0420 - accuracy - ETA: 1:14 - loss: 0.0420 - accuracy:  - ETA: 1:14 - loss: 0.0420 - accura - ETA: 1:14 - loss: 0.0420  - ETA: 1:13 - loss: 0.0420 - accuracy:  - ETA: 1:13 - loss: 0.0420 - accuracy:  - ETA: 1:13 - loss: 0.0420 - accuracy: 0. - ETA: 1:13 - loss: 0.0420 - accuracy: 0. - ETA: 1:13 - loss - ETA:  - ETA: 1s - los - ETA: 0s - loss: 0.0423 - accuracy: 0. - ETA: 0s - los\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04664 to 0.04500, saving model to models\\weights_gru_glove_wiki.best.hdf5\n",
      "4. Predictions completed\n",
      "\n",
      "Mean ROC-AUC: 0.9783651989946405\n",
      "5. Evaluation completed\n",
      "\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_gru_glove_wiki = run_nn(\n",
    "    struc='GRU',\n",
    "    model_fp=r'models/weights_gru_glove_wiki.best.hdf5',\n",
    "    train_df=train_df, test_df=test_df,\n",
    "    embed_fp=glove_wiki_fp,\n",
    "    embed_size=embed_size,\n",
    "    max_features=max_features,\n",
    "    maxlen=maxlen,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_labels=class_labels)\n",
    "scores_tracker['nn_gru_glove_wiki'] = score_gru_glove_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "american-defendant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxh88\\AppData\\Local\\Continuum\\anaconda3\\envs\\toxicc-env\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Embeddings generated\n",
      "\n",
      "3. Model generated (GRU)\n",
      "\n",
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 141s 31ms/step - loss: 0.0767 - accuracy: 0.8299 - val_loss: 0.0467 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04672, saving model to models\\weights_gru_glove_twitter.best.hdf5\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 139s 31ms/step - loss: 0.0421 - accuracy: 0.9536 - val_loss: 0.0452 - val_accuracy: 0.9928- loss: 0.0421 - ac - ETA: 0s - loss: 0.0421 - accuracy: 0.95 - ETA: 0s - loss: 0.0421 - accu - ETA: 0s - loss: 0.0421 - accuracy: 0. - ETA: 0s - loss: 0.0421 - accuracy - ETA: 0s - loss: 0.0421 - accuracy: \n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04672 to 0.04522, saving model to models\\weights_gru_glove_twitter.best.hdf5\n",
      "4. Predictions completed\n",
      "\n",
      "Mean ROC-AUC: 0.9796035068929402\n",
      "5. Evaluation completed\n",
      "\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_gru_glove_twitter = run_nn(\n",
    "    struc='GRU',\n",
    "    model_fp=r'models/weights_gru_glove_twitter.best.hdf5',\n",
    "    train_df=train_df, test_df=test_df,\n",
    "    embed_fp=glove_twitter_fp,\n",
    "    embed_size=embed_size,\n",
    "    max_features=max_features,\n",
    "    maxlen=maxlen,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_labels=class_labels)\n",
    "scores_tracker['nn_gru_glove_twitter'] = score_gru_glove_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-president",
   "metadata": {},
   "source": [
    "##### 2.4 GRU with Glove embedding and further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "forced-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy()\n",
    "test_df2 = test_df.copy()\n",
    "train_df2[comment_col] = train_df2[comment_col].apply(preprocess_text)\n",
    "test_df2[comment_col] = test_df2[comment_col].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "seasonal-property",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessed completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxh88\\AppData\\Local\\Continuum\\anaconda3\\envs\\toxicc-env\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Embeddings generated\n",
      "\n",
      "3. Model generated (GRU)\n",
      "\n",
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 142s 31ms/step - loss: 0.0773 - accuracy: 0.8303 - val_loss: 0.0458 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04584, saving model to models\\weights_gru_glove_twitter_clean.best.hdf5\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 139s 31ms/step - loss: 0.0412 - accuracy: 0.9431 - val_loss: 0.0452 - val_accuracy: 0.9936 1:18 - loss: 0.0406 - accuracy: 0.93 - ETA: 1:18 - - ETA: 1:17 - loss: 0.0406 - accuracy - ETA: 1:17 - loss: 0.0406 - accuracy:  - ETA: 1:17 - loss: 0.0406 - accuracy:  - ETA: 1:17 - loss: 0.0406 - accuracy:  - ETA: 1:17 - loss: 0.0406 - accuracy - ETA: 1:16 - loss: 0.0406 - accuracy - E - ETA: 1s - loss: 0.0412 - accuracy: 0.94 - ETA: 1s - loss: 0.0412  - E\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04584 to 0.04522, saving model to models\\weights_gru_glove_twitter_clean.best.hdf5\n",
      "4. Predictions completed\n",
      "\n",
      "Mean ROC-AUC: 0.9798407787592017\n",
      "5. Evaluation completed\n",
      "\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_gru_glove_twitter_clean = run_nn(\n",
    "    struc='GRU',\n",
    "    model_fp=r'models/weights_gru_glove_twitter_clean.best.hdf5',\n",
    "    train_df=train_df2, test_df=test_df2,\n",
    "    embed_fp=glove_twitter_fp,\n",
    "    embed_size=embed_size,\n",
    "    max_features=max_features,\n",
    "    maxlen=maxlen,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_labels=class_labels)\n",
    "scores_tracker['nn_gru_glove_twitter_clean'] = score_gru_glove_twitter_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
